# -*- coding: utf-8 -*-
"""Assignment2 DA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LuGi4do8GohqoDH-guFu0SpIjIor1zlq
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.impute import SimpleImputer
import numpy as np

life_df = pd.read_csv('Life_Expectancy_Data.csv')
#Here i displayed all the column names to find any irregularities like white spaces or
#columns starting with a Lower case, etc
life_df.columns

# Renaming columns
life_df.rename(columns={'Life expectancy ': 'Life expectancy', 'infant deaths': 'Infant deaths',
                        'percentage expenditure': 'Percentage expenditure', 'Measles ':'Measles',
                        ' BMI ':'BMI','under-five deaths ':'Under five deaths', 'Diphtheria ':'Diphtheria',
                        ' HIV/AIDS':'HIV/AIDS',' thinness  1-19 years':'Thinness  1-19 years',
                        ' thinness 5-9 years':'Thinness 5-9 years'}, inplace=True)

life_df.columns

life_df.head(20)

missing_values = life_df.isna().sum()
print("The number of missing values per column is:\n",missing_values)

#In order to get the mean values of the data and not the most-frequent (mode)
#I had to create a subset of the Data Frame that contains only numeric values
numeric_df = life_df.select_dtypes(include=['number'])

# Create an instance of SimpleImputer with strategy='mean' to fill missing values
imputer = SimpleImputer(strategy='mean')

# Fit the imputer to the data and transform to impute missing values with the mean
imputed_data = imputer.fit_transform(numeric_df)

# Convert the imputed data back to a DataFrame
df_imputed = pd.DataFrame(imputed_data, columns=numeric_df.columns)


print("\nDataFrame with missing values imputed using mean:")
print(df_imputed)

life_df.describe()

corr = life_df.select_dtypes(include='number').corr()
corr

from ctypes import alignment
plt.figure(figsize=(8,6))
ax = sb.heatmap(corr, linewidth=0.5)
plt.title('Correlation Heat map between numeric values from Life Expectancy Data\n',loc='center')
plt.tight_layout()
plt.show()
print('Based on this Heat map we can see a strong Correlation between some columns like: \n Life Expectancy and Schooling, Income composition of Resources and Life expectancy , Schooling and Income composition of Resources')

#Import Counter from collections
from collections import Counter
# Create a variable to hold just the life expectancy value from the data set
life_exp = life_df['Life expectancy']
# Count occurrences of each value
value_counts = Counter(life_exp)
# Extract top three values
top_five_values = [value for value, _ in value_counts.most_common(5)]

plt.figure(figsize=(8,6))
sb.histplot(data=life_df,x="Life expectancy", bins=40)
for value in top_five_values:
    plt.axvline(x=value, color='red')  # Add vertical lines for each top value

plt.figtext(0.5, -0.1, f'The top 5 Years of Life expectancy are {top_five_values}', horizontalalignment='center', verticalalignment='bottom', fontsize=10)

plt.show()

sb.violinplot(data=life_df, x="Status", y="Life expectancy")

life_df['Country'].unique()

countries = ['United States of America ','United Kingdom of Great Britain and Northern Ireland','Canada']
sub_df = life_df[(life_df['Year'].between(2000,2015)) & life_df['Country'].isin(countries)]

plt.figure(figsize=(8,6))
plt.subplot(1,2,1)
sb.violinplot(data=life_df, x="Status", y="Life expectancy")

plt.subplot(1,2,2)
sb.violinplot(data=sub_df, y="Life expectancy")
plt.xlabel("Canada, USA and UK \n Life expectancy between 2000 and 2015")

plt.tight_layout()
plt.show()

countries = ['Belgium', 'Brazil','Cameroon', 'Canada', 'China', 'France', 'Ghana', 'India',
             'United Kingdom of Great Britain and Northern Ireland','United States of America']

sub_df = life_df[life_df['Country'].isin(countries)]

plt.figure(figsize=(12,6))
sb.scatterplot(data=sub_df, x='Infant deaths', y="Life expectancy", hue='Country')

sub2012_df = life_df[(life_df['Year']==2012)]

correlation= sub2012_df["Life expectancy"].corr(sub2012_df['Schooling'])
print("Correlation in 2012 between Life expectancy and Schooling is : ",correlation)

plt.figure(figsize=(8,6))
sb.regplot(data=sub2012_df, x="Life expectancy", y="Schooling", scatter_kws={"color":"blue"}, line_kws={"color":"red"})

plt.title("Scatter plot of 2012 Life expectancy & Schooling with Regresion")
plt.figtext(0.5, -0.1, f'A correlation of {correlation} indicates a strong correlation between Schooling and Life expectancy \n wich means that when the Education level is higher, the life Expectancy tends to be longer', horizontalalignment='center', verticalalignment='bottom', fontsize=10)

plt.show()

text_df = pd.read_csv('text.csv')
text_df.head(5)

#First assign the value of the review column to a temporary list variable so i can manipulate it without affecting the original Data set
reviews = text_df['review'].str.lower()
reviews.head(5)

import nltk
from nltk.corpus import stopwords
from string import punctuation
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize

#download all the stopwords, punctuations and words for the lemmatizer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

#declare a variable that will hold all the stopwords and the list of punctuations
stop_words = set(stopwords.words('english')+list(punctuation))

#create a small function that will take a "review" as an argument
def cleantext(review):
  #if the element of the list is a string ...
    if isinstance(review, str):
      # it will tokenize the review paragraph and then compare word by word to check if the word exist in the
      #stop_words list, if that is true then it will join that to a space character and return the joined words.
       return ' '.join([word for word in word_tokenize(review) if  word not in stop_words])
    else:
       return str(review)

#Declare a variable that will hold the values returned by cleaned text function with the review as the argument
clean_reviews = [cleantext(review) for review in reviews]

#Create and add the clean_reviews column to the original Data set and assing the new clean_reviews values to it
text_df['clean_reviews'] = clean_reviews

#Display the first 5 rows
text_df['clean_reviews'].head(5)

#Initialize the WordNet lemmatizer
lemmatizer = WordNetLemmatizer()

# Lemmatize the clean_reviews column
text_df['clean_reviews'] = text_df['clean_reviews'].apply(lambda x: lemmatizer.lemmatize(x))

text_df['clean_reviews'].head(5)

#Remove any missing values
text_df['clean_reviews'] = text_df['clean_reviews'].dropna()

#Display the first five rows
text_df['clean_reviews'].head(5)

positive_df = text_df[text_df['label']=='pos'].drop(columns=['review'])
negative_df = text_df[text_df['label']=='neg'].drop(columns=['review'])

positive_df.head(5)

negative_df.head()

from sklearn.feature_extraction.text import TfidfVectorizer
positive_reviews = positive_df['clean_reviews'].tolist()
negative_reviews = negative_df['clean_reviews'].tolist()

# Initialize TfidfVectorizer
vectorizer = TfidfVectorizer()

tfidf_positive = vectorizer.fit_transform(positive_reviews)
tfidf_negative = vectorizer.fit_transform(negative_reviews)



